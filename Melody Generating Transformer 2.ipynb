{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UUEw2WFJmRj9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ki7FhyNdmjMk"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 512 # what is the maximum context length for predictions?\n",
    "max_iters = 2500\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 140\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.3\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEf6NZe3GIn8",
    "outputId": "96428008-5e92-492c-ab8a-bcd15fc171bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuCpr_OtoE-_",
    "outputId": "fc40cafe-111e-457e-e4db-6d193fc6ad0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x105684310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o-H9hrJfo4-y"
   },
   "outputs": [],
   "source": [
    "with open('../finalAssignment_musicDataset/Modified Code/inputMelodiesAugmented.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Dataset - ./inputMelodiesAugmented.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../finalAssignment_musicDataset/Modified Code/inputMelodiesAugmented.txt', 'r') as file:\n",
    "    melodies = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "melody_tokens = [melody.strip().split() for melody in melodies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_augmented_data(melody_tokens):\n",
    "    pitches = []\n",
    "    durations = []\n",
    "    for token in melody_tokens:\n",
    "        if token.startswith(\"R\"):\n",
    "            pitches.append(\"R\")\n",
    "            durations.append(int(token[2:-1]))\n",
    "        else:\n",
    "            pitch, duration = token.split(\"(\")\n",
    "            pitches.append(pitch)\n",
    "            durations.append(int(duration[:-1]))\n",
    "    return pitches, durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = []\n",
    "for melody in melodies:\n",
    "    melody_tokens = melody.strip().split()\n",
    "    pitches, durations = parse_augmented_data(melody_tokens)\n",
    "    parsed_data.append((pitches, durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_vocab = sorted(set(pitch for pitches, _ in parsed_data for pitch in pitches))\n",
    "duration_vocab = sorted(set(duration for _, durations in parsed_data for duration in durations))\n",
    "\n",
    "pitch_to_id = {pitch: idx for idx, pitch in enumerate(pitch_vocab)}\n",
    "id_to_pitch = {idx: pitch for pitch, idx in pitch_to_id.items()}\n",
    "\n",
    "duration_to_id = {duration: idx for idx, duration in enumerate(duration_vocab)}\n",
    "id_to_duration = {idx: duration for duration, idx in duration_to_id.items()}\n",
    "\n",
    "pitch_vocab_size = len(pitch_vocab)\n",
    "duration_vocab_size = len(duration_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(parsed_data):\n",
    "    pitch_encoded = [pitch_to_id[p] for p in parsed_data[0]]\n",
    "    duration_encoded = [duration_to_id[d] for d in parsed_data[1]]\n",
    "    return pitch_encoded, duration_encoded\n",
    "\n",
    "encoded_data = [encode_sequence(data) for data in parsed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(pitch_output, duration_output):\n",
    "    decoded_sequence = []\n",
    "    pitch_output = pitch_output.cpu().numpy().flatten()\n",
    "    duration_output = duration_output.cpu().numpy().flatten()\n",
    "\n",
    "    for pitch_id, duration_id in zip(pitch_output, duration_output):\n",
    "        pitch = id_to_pitch.get(pitch_id, \"Unknown\")\n",
    "        duration = id_to_duration.get(duration_id, \"Unknown\")\n",
    "        \n",
    "        if pitch == \"R\":\n",
    "            decoded_sequence.append(f\"R({duration})\")\n",
    "        else:\n",
    "            decoded_sequence.append(f\"{pitch}({duration})\")\n",
    "    \n",
    "    return decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_data = torch.tensor([p for pitch_seq, _ in encoded_data for p in pitch_seq], dtype=torch.long)\n",
    "duration_data = torch.tensor([d for _, duration_seq in encoded_data for d in duration_seq], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(pitch_data))  # first 90% for training, rest for validation\n",
    "train_pitch_data = pitch_data[:n]\n",
    "val_pitch_data = pitch_data[n:]\n",
    "train_duration_data = duration_data[:n]\n",
    "val_duration_data = duration_data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1o4d70RWpTt6"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data_pitch = train_pitch_data if split == 'train' else val_pitch_data\n",
    "    data_duration = train_duration_data if split == 'train' else val_duration_data\n",
    "    ix = torch.randint(len(data_pitch) - block_size, (batch_size,))\n",
    "    \n",
    "    pitch_batch = torch.stack([data_pitch[i:i+block_size] for i in ix])\n",
    "    duration_batch = torch.stack([data_duration[i:i+block_size] for i in ix])\n",
    "\n",
    "    pitch_target = torch.stack([data_pitch[i+1:i+block_size+1] for i in ix])\n",
    "    duration_target = torch.stack([data_duration[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return pitch_batch.to(device), duration_batch.to(device), (pitch_target.to(device), duration_target.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7S5fCaufpT3O"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Hdmno5nGpT8-"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=True)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=True)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=True)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4L2rwuA2pUB1"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uJG24-EfpUGI"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KKkOrP04pUKH"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fycHKAvZpUVC"
   },
   "outputs": [],
   "source": [
    "class GPTMelodyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pitch_embedding = nn.Embedding(pitch_vocab_size, n_embd)\n",
    "        self.duration_embedding = nn.Embedding(duration_vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.pitch_weight = nn.Parameter(torch.ones(1))\n",
    "        self.duration_weight = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.pitch_head = nn.Linear(n_embd, pitch_vocab_size)\n",
    "        self.duration_head = nn.Linear(n_embd, duration_vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, pitch_idx, duration_idx, targets=None):\n",
    "        B, T = pitch_idx.shape\n",
    "        pitch_emb = self.pitch_embedding(pitch_idx)\n",
    "        duration_emb = self.duration_embedding(duration_idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
    "\n",
    "        x = (self.pitch_weight * pitch_emb) + (self.duration_weight * duration_emb) + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        pitch_logits = self.pitch_head(x)\n",
    "        duration_logits = self.duration_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return pitch_logits, duration_logits, None\n",
    "\n",
    "        pitch_target, duration_target = targets\n",
    "        pitch_loss = F.cross_entropy(pitch_logits.view(-1, pitch_logits.size(-1)), pitch_target.view(-1))\n",
    "        duration_loss = F.cross_entropy(duration_logits.view(-1, duration_logits.size(-1)), duration_target.view(-1))\n",
    "        \n",
    "        loss = 0.7 * pitch_loss + 0.3 * duration_loss\n",
    "        return pitch_logits, duration_logits, loss\n",
    "\n",
    "    def generate(self, context, max_new_tokens):\n",
    "        pitch_idx = context.clone()\n",
    "        duration_idx = context.clone()\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            pitch_cond = pitch_idx[:, -block_size:]\n",
    "            duration_cond = duration_idx[:, -block_size:]\n",
    "\n",
    "            pitch_logits, duration_logits, _ = self(pitch_cond, duration_cond)\n",
    "\n",
    "            pitch_logits = pitch_logits[:, -1, :]\n",
    "            duration_logits = duration_logits[:, -1, :]\n",
    "\n",
    "            pitch_probs = F.softmax(pitch_logits, dim=-1)\n",
    "            duration_probs = F.softmax(duration_logits, dim=-1)\n",
    "\n",
    "            pitch_next = torch.multinomial(pitch_probs, num_samples=1)\n",
    "            duration_next = torch.multinomial(duration_probs, num_samples=1)\n",
    "\n",
    "            pitch_idx = torch.cat((pitch_idx, pitch_next), dim=1)\n",
    "            duration_idx = torch.cat((duration_idx, duration_next), dim=1)\n",
    "\n",
    "        return pitch_idx, duration_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0RtwLVsqyH-",
    "outputId": "8c28ff37-6689-4bed-d5c3-dd1a9f905ada"
   },
   "outputs": [],
   "source": [
    "model = GPTMelodyModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.2188, val loss 5.1922, train perplexity 184.7104, val perplexity 179.8593\n",
      "Current Learning Rate: 0.000300\n",
      "step 500: train loss 1.0176, val loss 1.0451, train perplexity 2.7667, val perplexity 2.8438\n",
      "Current Learning Rate: 0.000271\n",
      "step 1000: train loss 0.9134, val loss 0.9510, train perplexity 2.4928, val perplexity 2.5884\n",
      "Current Learning Rate: 0.000196\n",
      "step 1500: train loss 0.8706, val loss 0.9151, train perplexity 2.3884, val perplexity 2.4969\n",
      "Current Learning Rate: 0.000103\n",
      "step 2000: train loss 0.8509, val loss 0.8989, train perplexity 2.3418, val perplexity 2.4569\n",
      "Current Learning Rate: 0.000029\n",
      "step 2499: train loss 0.8453, val loss 0.8932, train perplexity 2.3288, val perplexity 2.4430\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses = {}\n",
    "            perplexities = {}\n",
    "            for split in ['train', 'val']:\n",
    "                loss = torch.tensor([model(*get_batch(split))[-1].item() for _ in range(eval_iters)]).mean()\n",
    "                losses[split] = loss\n",
    "                perplexities[split] = math.exp(loss)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, \"\n",
    "                 f\"train perplexity {perplexities['train']:.4f}, val perplexity {perplexities['val']:.4f}\")\n",
    "        model.train()\n",
    "\n",
    "    pitch_batch, duration_batch, targets = get_batch('train')\n",
    "    pitch_logits, duration_logits, loss = model(pitch_batch, duration_batch, targets)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        print(f\"Current Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "pitch_output, duration_output = model.generate(context, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Melody: A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) A2(1) R(1200) D4(181) R(25546) D6(181) R(10) A3(364) R(29) F3(181) R(3) f3(6380) R(5) A3(90) R(5) D4(181) R(10) f3(181) R(10) D4(90) R(5) c3(272) R(783) C5(1276) R(269) f3(181) R(10) A3(181) R(2117) c4(7) R(242) D4(272) R(9792) B3(546) R(701) A3(272) R(2867) R(408) B5(181) R(202) G4(181) R(10) F3(546) R(25316) D3(1392) R(794) A3(95) R(96) A3(181) R(10) F4(181) R(10) A3(181) R(10) A3(181) R(10) F3(181) R(10) E6(455) R(39) B4(181) R(10) d4(181) R(10) D4(181) R(10) E4(181) R(10) F4(181) R(10) f4(911) R(1584) B3(90) R(293) B3(181) R(10) a3(181) R(10) B3(181) R(10) E4(181) R(10) A3(272) R(111) F4(90) R(5) B3(364) R(20) G4(181) R(10) D4(181) R(202) D4(181) R(202) D4(181) R(10) a3(181) R(10) C3(181) R(10) B3(90) R(5) B3(181) R(10) f4(181) R(106) E3(90) R(5) f4(90) R(864) B5(181) R(202) a4(181) R(1162) F4(181) R(202) E4(181) R(10) F4(364) R(5) A3(1276) R(12457) G4(432) R(430) f4(181) R(10) F3(272) R(1469) a3(90) R(5) E4(181) R(10) f4(181) R(202) f3(181) R(10) G4(181) R(10) d4(181) R(10) E4(181) R(10) F4(120) R(7) E4(181) R(10) a3(181) R(10) F4(181) R(10) E4(181) R(10) g4(181) R(10) a3(181) R(202) E4(181) R(10) f4(181) R(10) D4(181) R(10) F4(90) R(101) D4(181) R(10) f4(364) R(20) f4(181) R(10) A4(181) R(10) g4(181) R(10) E4(181) R(10) F4(181) R(10) E4(181) R(10) E4(364) R(20) g4(181) R(10) A3(181) R(298) D4(90) R(485) a3(90) R(5) F4(181) R(10) a4(181) R(106) f4(181) R(10) f4(181) R(586) D4(90) R(5) D4(181) R(10) c4(181) R(10) E4(181) R(10) D4(181) R(970) f4(181) R(10) E4(546) R(332) C4(90) R(5) f4(272) R(15) f4(181) R(586) a3(181) R(10) G4(181) R(10) a4(181) R(10) D4(181) R(586) F4(181) R(10) g4(181) R(10) a3(181) R(10) D4(181) R(106) A3(90) R(5) f4(90) R(5) a3(181) R(10) F4(181) R(10) a4(95) R(288) F4(90) R(5) G3(181) R(10) c4(181) R(10) c4(181) R(10) E4(181) R(10) F4(181) R(10) f3(364) R(212) F4(181) R(10) F4(181) R(10) a4(95) R(288) a3(181) R(106) a3(90) R(5) g4(181) R(10) A4(181) R(10) F4(181) R(10) G3(181) R(202) g4(181) R(202) F4(181) R(10) F4(181) R(10) F4(181) R(10) D4(181) R(10) F4(181) R(10) a3(181) R(106) a3(90) R(197) a4(90) R(5) a3(364) R(116) f3(90) R(5) a3(181) R(10) C3(181) R(10) F4(272) R(15) B3(181) R(10) A3(181) R(10) G3(181) R(10) D4(728) R(231) f4(181) R(10) G4(181) R(10) F4(181) R(106) F4(90) R(101) a3(181) R(10) d4(181) R(298) E4(181) R(10) g3(181) R(202) F4(181) R(10) D4(181) R(202) a3(181) R(10) E4(181) R(10) a3(90) R(5) B3(90) R(101) a3(90) f4(5) a3(90) R(5) F4(90) R(5) f3(181) R(10) A4(181) R(10) E4(364) R(596) a4(181) R(10) A4(364) R(20) F4(181) R(10) d3(364) R(20) D4(181) R(10) a3(181) R(10) F4(181) R(10) a3(90) R(5) a3(181) R(10) B3(181) R(10) a3(181) R(10) F4(546) R(221) F4(181) R(10) g4(181) R(10) D4(181) R(10) g4(181) R(10) d4(546) R(4609) F4(614) R(797) f4(181) R(202) a3(181) R(10) A3(181) R(10) C3(364) R(116) D4(455) R(792) G4(911) R(48) F4(181) R(10) d5(181) R(10) F4(181) R(394) F4(181) R(10) d4(181) R(10) D4(181) R(10) g3(181) R(10) F4(546) R(29) F4(181) R(10) d4(181) R(10) D4(181) R(10) D4(728) R(423) f4(181) R(10) F4(181) R(394) F4(181) R(202) F4(181) R(10) f4(181) R(10) f4(181) R(10) F4(181) R(10) C3(181) R(10) a3(181) R(10) d4(181) R(10) d4(181) R(10) a3(181) R(202) F4(181) R(10) F4(181) R(106) F4(181) R(10) F4(181) R(394) a3(181) R(10) C3(364) R(20) G4(181) R(106) c4(181) R(10) F4(181) R(10) a3(364) R(788) a3(181) R(10) a3(181) R(10) G4(181) R(10) a3(181) R(10) F4(181) R(10) F4(181) R(10) a3(181) R(586) a3(181) R(106) f3(90) R(5) a3(90) R(5) a3(181) R(10) a3(181) R(10) a3(546) R(413) f4(181) R(394) c3(364) R(20) D4(181) R(10) a3(364) R(20)\n"
     ]
    }
   ],
   "source": [
    "decoded_melody = decode_sequence(pitch_output, duration_output)\n",
    "print(\"Decoded Melody:\", \" \".join(decoded_melody))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "max_new_tokens = 500\n",
    "\n",
    "generated_sequences = []\n",
    "for _ in range(N):\n",
    "    context = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "    pitch_output, duration_output = model.generate(context, max_new_tokens=max_new_tokens)\n",
    "    pitch_output_sliced=pitch_output.squeeze()[250:]\n",
    "    duration_output_sliced=pitch_output.squeeze()[250:]\n",
    "    generated_sequences.append((pitch_output_sliced.tolist(), duration_output_sliced.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_similarity(seq1, seq2):\n",
    "    set1, set2 = set(seq1), set(seq2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_jaccard_similarity(sequences):\n",
    "    total_similarity = 0\n",
    "    count = 0\n",
    "    for i in range(len(sequences)):\n",
    "        for j in range(i + 1, len(sequences)):\n",
    "            similarity = calculate_jaccard_similarity(sequences[i], sequences[j])\n",
    "            total_similarity += similarity\n",
    "            count += 1\n",
    "    return total_similarity / count if count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch Jaccard Similarity): 0.5565\n",
      "Duration Jaccard Similarity): 0.5565\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pitch Jaccard Similarity): {pitch_jaccard:.4f}\")\n",
    "print(f\"Duration Jaccard Similarity): {duration_jaccard:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
